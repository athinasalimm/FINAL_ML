{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from src.utils import filtrar_meses, verificar_columnas_y_tipos, estandarizar_nombres_columnas\n",
    "from src.utils import funciones \n",
    "from src.utils import verificar_consistencia_coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils import corregir_longitudes_inconsistentes\n",
    "\n",
    "def procesar_recorridos_anio(ruta_csv_entrada, ruta_csv_salida, meses_a_eliminar, col_origen=\"fecha_origen_recorrido\", col_destino=\"fecha_destino_recorrido\"):\n",
    "\n",
    "    df = pd.read_csv(ruta_csv_entrada)\n",
    "\n",
    "    df_filtrado, filas_eliminadas_origen = filtrar_meses(df, col_origen, meses_a_eliminar)\n",
    "    print(f\"Se eliminaron {filas_eliminadas_origen} filas por fecha de origen.\")\n",
    "\n",
    "    df_filtrado, filas_eliminadas_destino = filtrar_meses(df_filtrado, col_destino, meses_a_eliminar)\n",
    "    print(f\"Se eliminaron {filas_eliminadas_destino} filas por fecha de destino.\")\n",
    "\n",
    "    total_eliminadas = filas_eliminadas_origen + filas_eliminadas_destino\n",
    "\n",
    "    df_filtrado.to_csv(ruta_csv_salida, index=False)\n",
    "\n",
    "    return df_filtrado, total_eliminadas\n",
    "\n",
    "def limpiar_y_guardar_recorridos():\n",
    "    a√±os_columnas_a_dropear = {\n",
    "        2020: [],\n",
    "        2021: [\"G√©nero\"],\n",
    "        2022: [\"X\"],\n",
    "        2023: []\n",
    "    }\n",
    "\n",
    "    for a√±o, columnas in a√±os_columnas_a_dropear.items():\n",
    "        ruta_entrada = f\"data/recorridos/raw/trips_{a√±o}.csv\"\n",
    "        ruta_salida = f\"data/recorridos/processed/trips_{a√±o}_pr.csv\"\n",
    "        \n",
    "        df = pd.read_csv(ruta_entrada, index_col=0)\n",
    "        \n",
    "        if columnas:\n",
    "            df = df.drop(columns=[col for col in columnas if col in df.columns])\n",
    "        \n",
    "        df = df.reset_index(drop=True)\n",
    "        df.to_csv(ruta_salida, index=False)\n",
    "\n",
    "\n",
    "def forzar_tipos(df, columnas, tipo):\n",
    "    for col in columnas:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(tipo)\n",
    "    return df\n",
    "\n",
    "def limpiar_y_normalizar_archivos(archivos, funciones):\n",
    "    anios = [2020, 2021, 2022, 2023]\n",
    "\n",
    "    estandarizar_nombres_columnas(archivos)\n",
    "\n",
    "    for anio in [2020, 2023]:\n",
    "        ruta = f\"data/recorridos/processed/trips_{anio}_pr.csv\"\n",
    "        df = pd.read_csv(ruta)\n",
    "        df = df.dropna()\n",
    "        df.to_csv(ruta, index=False)\n",
    "\n",
    "    for anio in anios:\n",
    "        print(f\"Procesando a√±o {anio}...\")\n",
    "        ruta = f\"data/recorridos/processed/trips_{anio}_pr.csv\"\n",
    "        df = pd.read_csv(ruta)\n",
    "        df_limpio = funciones[str(anio)](df)\n",
    "        df_limpio = forzar_tipos(df_limpio, [\"id_usuario\"], 'float64')\n",
    "        df_limpio.to_csv(ruta, index=False)\n",
    "\n",
    "    verificar_columnas_y_tipos(archivos)\n",
    "\n",
    "def corregir_coordenadas_con_base_2024(path_2024, archivos_a_corregir):\n",
    "    df_2024 = pd.read_csv(path_2024)\n",
    "    coords_2024 = {}\n",
    "\n",
    "    for rol in [\"origen\", \"destino\"]:\n",
    "        id_col = f\"id_estacion_{rol}\"\n",
    "        lat_col = f\"lat_estacion_{rol}\"\n",
    "        lon_col = f\"long_estacion_{rol}\"\n",
    "\n",
    "        for _, row in df_2024.iterrows():\n",
    "            est_id = row[id_col]\n",
    "            lat = row[lat_col]\n",
    "            lon = row[lon_col]\n",
    "            if pd.notna(est_id) and pd.notna(lat) and pd.notna(lon):\n",
    "                coords_2024[est_id] = (lat, lon)\n",
    "\n",
    "    for anio, path in archivos_a_corregir.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"‚ö†Ô∏è No se encontr√≥ el archivo de {anio}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        for rol in [\"origen\", \"destino\"]:\n",
    "            id_col = f\"id_estacion_{rol}\"\n",
    "            lat_col = f\"lat_estacion_{rol}\"\n",
    "            lon_col = f\"long_estacion_{rol}\"\n",
    "\n",
    "            def corregir_coord(row):\n",
    "                est_id = row[id_col]\n",
    "                if est_id in coords_2024:\n",
    "                    lat, lon = coords_2024[est_id]\n",
    "                    row[lat_col] = lat\n",
    "                    row[lon_col] = lon\n",
    "                return row\n",
    "\n",
    "            df = df.apply(corregir_coord, axis=1)\n",
    "\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"‚úÖ Archivo actualizado y sobrescrito: {path}\")\n",
    "\n",
    "from src.utils import corregir_latitud_estacion\\\n",
    "\n",
    "def corregir_estaciones_2020():\n",
    "    path_2020 = \"data/recorridos/processed/trips_2020_pr.csv\"\n",
    "    df_2020 = pd.read_csv(path_2020)\n",
    "    df_2020 = corregir_latitud_estacion(df_2020, est_id=240)\n",
    "    df_2020.to_csv(path_2020, index=False)\n",
    "    corregir_longitudes_inconsistentes(path_2020)\n",
    "\n",
    "def verificar_coordenadas_todos_los_anios(anios = [], test: bool = False, path_csv: str = None):\n",
    "    if not test: \n",
    "        for anio in anios:\n",
    "            path = f\"data/recorridos/processed/trips_{anio}_pr.csv\"\n",
    "            verificar_consistencia_coordenadas(path, anio=anio)\n",
    "    if test: \n",
    "        verificar_consistencia_coordenadas(path_csv, test=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def construir_diccionario_estaciones(paths_descendentes):\n",
    "    estaciones_dict = {}\n",
    "\n",
    "    for path in paths_descendentes:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"‚ö†Ô∏è Archivo no encontrado: {path}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        for rol in [\"origen\", \"destino\"]:\n",
    "            id_col = f\"id_estacion_{rol}\"\n",
    "            lat_col = f\"lat_estacion_{rol}\"\n",
    "            lon_col = f\"long_estacion_{rol}\"\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                est_id = row[id_col]\n",
    "                lat = row[lat_col]\n",
    "                lon = row[lon_col]\n",
    "\n",
    "                if pd.notna(est_id) and pd.notna(lat) and pd.notna(lon):\n",
    "                    if est_id not in estaciones_dict:\n",
    "                        estaciones_dict[est_id] = (lat, lon)\n",
    "\n",
    "    return estaciones_dict\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "\n",
    "def asignar_barrios_a_datasets(df_estaciones, anios=None, test=False, path_test=None, path_shapefile=\"data/barrios/barrios.shp\"):\n",
    "    \"\"\"\n",
    "    Enriquece datasets de viajes agregando los barrios de origen y destino seg√∫n las estaciones.\n",
    "\n",
    "    Par√°metros:\n",
    "    - df_estaciones: DataFrame con columnas ['id_estacion', 'lat', 'lon']\n",
    "    - anios: lista de a√±os a procesar (si test=False)\n",
    "    - test: si es True, se aplica solo al archivo en path_test\n",
    "    - path_test: path a un CSV de viajes para modo test\n",
    "    - path_shapefile: ruta al archivo de pol√≠gonos de barrios\n",
    "    \"\"\"\n",
    "    # ========================\n",
    "    # üìç 1. Cargar shapefile de barrios\n",
    "    # ========================\n",
    "    barrios_gdf = gpd.read_file(path_shapefile)\n",
    "    if \"nombre\" in barrios_gdf.columns and \"barrio\" not in barrios_gdf.columns:\n",
    "        barrios_gdf = barrios_gdf.rename(columns={\"nombre\": \"barrio\"})\n",
    "\n",
    "    # ========================\n",
    "    # üìç 2. Join espacial: estaciones ‚Üí barrios\n",
    "    # ========================\n",
    "    df_estaciones[\"geometry\"] = df_estaciones.apply(lambda row: Point(row[\"lon\"], row[\"lat\"]), axis=1)\n",
    "    estaciones_gdf = gpd.GeoDataFrame(df_estaciones, geometry=\"geometry\", crs=barrios_gdf.crs)\n",
    "\n",
    "    estaciones_con_barrios = gpd.sjoin(estaciones_gdf, barrios_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    # Guardar CSV intermedio (opcional)\n",
    "    estaciones_con_barrios[[\"id_estacion\", \"lat\", \"lon\", \"barrio\"]].to_csv(\"data/estaciones_con_barrios.csv\", index=False)\n",
    "\n",
    "    # Diccionario id_estacion ‚Üí barrio\n",
    "    mapa_barrio = estaciones_con_barrios.set_index(\"id_estacion\")[\"barrio\"].to_dict()\n",
    "\n",
    "    # Correcciones manuales\n",
    "    mapa_barrio[111] = \"PUERTO MADERO\"\n",
    "    mapa_barrio[541] = \"PALERMO\"\n",
    "\n",
    "    # ========================\n",
    "    # üìç 3. Enriquecer datasets\n",
    "    # ========================\n",
    "    if test:\n",
    "        if path_test is None or not os.path.exists(path_test):\n",
    "            print(\"‚ö†Ô∏è Path inv√°lido o no encontrado para test.\")\n",
    "            return\n",
    "        df = pd.read_csv(path_test)\n",
    "        df[\"barrio_origen\"] = df[\"id_estacion_origen\"].map(mapa_barrio)\n",
    "        df[\"barrio_destino\"] = df[\"id_estacion_destino\"].map(mapa_barrio)\n",
    "        df.to_csv(path_test, index=False)\n",
    "        print(f\"üß™ Archivo enriquecido (modo test): {path_test}\")\n",
    "        return\n",
    "\n",
    "    if anios is None:\n",
    "        print(\"‚ö†Ô∏è No se especificaron a√±os para procesar.\")\n",
    "        return\n",
    "\n",
    "    for anio in anios:\n",
    "        path = f\"data/recorridos/processed/trips_{anio}_pr.csv\"\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"‚ö†Ô∏è Archivo no encontrado: {path}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        df[\"barrio_origen\"] = df[\"id_estacion_origen\"].map(mapa_barrio)\n",
    "        df[\"barrio_destino\"] = df[\"id_estacion_destino\"].map(mapa_barrio)\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"‚úÖ Enriquecido con barrios: {anio}\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def obtener_o_construir_df_estaciones(lista_paths, path_guardado=\"data/estaciones_unico.csv\"):\n",
    "    if os.path.exists(path_guardado):\n",
    "        print(\"üìÇ Usando df_estaciones ya guardado.\")\n",
    "        return pd.read_csv(path_guardado)\n",
    "\n",
    "    print(\"‚öôÔ∏è Construyendo df_estaciones desde archivos...\")\n",
    "    estaciones_dict = {}\n",
    "\n",
    "    for path in lista_paths:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"‚ö†Ô∏è Archivo no encontrado: {path}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        for rol in [\"origen\", \"destino\"]:\n",
    "            id_col = f\"id_estacion_{rol}\"\n",
    "            lat_col = f\"lat_estacion_{rol}\"\n",
    "            lon_col = f\"long_estacion_{rol}\"\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                est_id = row[id_col]\n",
    "                lat = row[lat_col]\n",
    "                lon = row[lon_col]\n",
    "\n",
    "                if pd.notna(est_id) and pd.notna(lat) and pd.notna(lon):\n",
    "                    if est_id not in estaciones_dict:\n",
    "                        estaciones_dict[est_id] = (lat, lon)\n",
    "\n",
    "    df_estaciones = pd.DataFrame([\n",
    "        {\"id_estacion\": est_id, \"lat\": lat, \"lon\": lon}\n",
    "        for est_id, (lat, lon) in estaciones_dict.items()\n",
    "    ])\n",
    "\n",
    "    # Guardar el CSV para futuros usos\n",
    "    os.makedirs(os.path.dirname(path_guardado), exist_ok=True)\n",
    "    df_estaciones.to_csv(path_guardado, index=False)\n",
    "    print(f\"‚úÖ df_estaciones guardado en {path_guardado}\")\n",
    "\n",
    "    return df_estaciones\n",
    "\n",
    "def analyze_barrios(path_shapefile=\"data/barrios/barrios.shp\"):\n",
    "    \"\"\"\n",
    "    Analiza y muestra informaci√≥n general del shapefile de barrios.\n",
    "    \n",
    "    Par√°metros:\n",
    "    - path_shapefile: ruta al archivo .shp\n",
    "    \n",
    "    Salida:\n",
    "    - Imprime columnas, cantidad de barrios √∫nicos y sus nombres.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        barrios_gdf = gpd.read_file(path_shapefile)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al leer el shapefile: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"üß© Columnas disponibles en el shapefile:\")\n",
    "    print(barrios_gdf.columns.tolist())\n",
    "\n",
    "    if \"nombre\" not in barrios_gdf.columns:\n",
    "        print(\"‚ö†Ô∏è La columna 'nombre' no existe en el shapefile.\")\n",
    "        return\n",
    "\n",
    "    barrios_unicos = sorted(barrios_gdf[\"nombre\"].dropna().unique())\n",
    "    total = len(barrios_unicos)\n",
    "\n",
    "    print(f\"\\nüìä Cantidad de barrios √∫nicos en el shapefile: {total}\")\n",
    "    print(\"\\nüó∫Ô∏è Barrios:\")\n",
    "    print(barrios_unicos)\n",
    "    print(f\"\\nüßÆ Total de barrios: {total}\")\n",
    "\n",
    "\n",
    "def analizar_presencia_estaciones(archivos, anios):\n",
    "    estaciones_por_anio = {}\n",
    "\n",
    "    for archivo, anio in zip(archivos, anios):\n",
    "        if not os.path.exists(archivo):\n",
    "            print(f\"‚ö†Ô∏è Archivo no encontrado para el a√±o {anio}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(archivo)\n",
    "        estaciones_origen = df['id_estacion_origen'].dropna().unique()\n",
    "        estaciones_destino = df['id_estacion_destino'].dropna().unique()\n",
    "        estaciones = set(estaciones_origen).union(set(estaciones_destino))\n",
    "        estaciones_por_anio[anio] = estaciones\n",
    "\n",
    "    todas_las_estaciones = sorted(set.union(*estaciones_por_anio.values()))\n",
    "    tabla_presencia = pd.DataFrame(index=todas_las_estaciones)\n",
    "\n",
    "    for anio in anios:\n",
    "        estaciones = estaciones_por_anio.get(anio, set())\n",
    "        tabla_presencia[anio] = [\"‚úì\" if est in estaciones else \"\" for est in tabla_presencia.index]\n",
    "\n",
    "    tabla_presencia[\"Anios_presente\"] = tabla_presencia[anios].apply(lambda row: sum(cell == \"‚úì\" for cell in row), axis=1)\n",
    "    tabla_presencia = tabla_presencia.sort_index()\n",
    "\n",
    "    print(\"\\nüìã Presencia de estaciones por a√±o (por ID, ordenadas por cantidad de a√±os presentes):\")\n",
    "    print(tabla_presencia)\n",
    "\n",
    "    print(\"\\nüìà Cantidad de estaciones por a√±o:\")\n",
    "    for anio in anios:\n",
    "        cantidad = len(estaciones_por_anio.get(anio, set()))\n",
    "        print(f\"  - {anio}: {cantidad} estaciones\")\n",
    "\n",
    "    return tabla_presencia, estaciones_por_anio\n",
    "\n",
    "\n",
    "def mostrar_estaciones_faltantes_en_2024(estaciones_por_anio):\n",
    "    print(\"\\nüîç Estaciones que aparec√≠an en a√±os anteriores pero NO en 2024:\")\n",
    "\n",
    "    estaciones_2024 = estaciones_por_anio.get(2024, set())\n",
    "\n",
    "    for anio in [2020, 2021, 2022, 2023]:\n",
    "        estaciones_anio = estaciones_por_anio.get(anio, set())\n",
    "        solo_en_anio = sorted(estaciones_anio - estaciones_2024)\n",
    "        print(f\"\\n‚û°Ô∏è Estaciones en {anio} pero NO en 2024 ({len(solo_en_anio)}):\")\n",
    "        print(solo_en_anio)\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "\n",
    "def estaciones_graf():\n",
    "    # ----------------------\n",
    "    # COLORES HARDCODEADOS\n",
    "    # ----------------------\n",
    "    colores_barrios = [\n",
    "        \"red\", \"green\", \"blue\", \"darkorange\", \"purple\", \"cyan\", \"magenta\", \"gold\",\n",
    "        \"orchid\", \"tomato\", \"lightpink\", \"deepskyblue\", \"chartreuse\", \"firebrick\", \"sienna\", \"dodgerblue\",\n",
    "        \"plum\", \"turquoise\", \"slateblue\", \"darkgreen\", \"hotpink\", \"indigo\", \"salmon\", \"navy\",\n",
    "        \"darkred\", \"chocolate\", \"crimson\", \"teal\", \"coral\", \"darkviolet\", \"mediumorchid\", \"mediumseagreen\",\n",
    "        \"darkmagenta\", \"mediumblue\", \"olivedrab\", \"slategray\", \"seagreen\", \"deeppink\", \"steelblue\", \"brown\",\n",
    "        \"orangered\", \"forestgreen\", \"darkcyan\", \"violet\", \"palevioletred\", \"blueviolet\", \"darkslateblue\", \"limegreen\"\n",
    "    ]\n",
    "\n",
    "    # ----------------------\n",
    "    # CARGAR BARRIOS\n",
    "    # ----------------------\n",
    "    barrios_gdf = gpd.read_file(\"data/barrios/barrios.shp\").to_crs(epsg=4326)\n",
    "    barrios_unicos = sorted(barrios_gdf[\"nombre\"].unique())\n",
    "    assert len(colores_barrios) == len(barrios_unicos), \"La cantidad de colores no coincide con la de barrios\"\n",
    "    color_dict = {barrio: colores_barrios[i] for i, barrio in enumerate(barrios_unicos)}\n",
    "\n",
    "    # ----------------------\n",
    "    # CARGAR ESTACIONES DE TODOS LOS A√ëOS\n",
    "    # ----------------------\n",
    "    archivos = [\n",
    "        \"data/recorridos/processed/trips_2024_pr.csv\",\n",
    "        \"data/recorridos/processed/trips_2023_pr.csv\",\n",
    "        \"data/recorridos/processed/trips_2022_pr.csv\",\n",
    "        \"data/recorridos/processed/trips_2021_pr.csv\",\n",
    "        \"data/recorridos/processed/trips_2020_pr.csv\"\n",
    "    ]\n",
    "\n",
    "    estaciones_dict = {}\n",
    "    for path in archivos:\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        df = pd.read_csv(path)\n",
    "        for rol in [\"origen\", \"destino\"]:\n",
    "            for _, row in df.iterrows():\n",
    "                est_id = row[f\"id_estacion_{rol}\"]\n",
    "                lat = row[f\"lat_estacion_{rol}\"]\n",
    "                lon = row[f\"long_estacion_{rol}\"]\n",
    "                if pd.notna(est_id) and pd.notna(lat) and pd.notna(lon):\n",
    "                    if est_id not in estaciones_dict:\n",
    "                        estaciones_dict[est_id] = (lat, lon)\n",
    "\n",
    "    df_estaciones_todas = pd.DataFrame([\n",
    "        {\"id_estacion\": est_id, \"lat\": lat, \"lon\": lon}\n",
    "        for est_id, (lat, lon) in estaciones_dict.items()\n",
    "    ])\n",
    "    df_estaciones_todas[\"geometry\"] = df_estaciones_todas.apply(lambda row: Point(row[\"lon\"], row[\"lat\"]), axis=1)\n",
    "    gdf_estaciones_todas = gpd.GeoDataFrame(df_estaciones_todas, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    # ----------------------\n",
    "    # CARGAR ESTACIONES 2024\n",
    "    # ----------------------\n",
    "    estaciones_2024 = {}\n",
    "    df_2024 = pd.read_csv(\"data/recorridos/processed/trips_2024_pr.csv\")\n",
    "    for rol in [\"origen\", \"destino\"]:\n",
    "        for _, row in df_2024.iterrows():\n",
    "            est_id = row[f\"id_estacion_{rol}\"]\n",
    "            lat = row[f\"lat_estacion_{rol}\"]\n",
    "            lon = row[f\"long_estacion_{rol}\"]\n",
    "            if pd.notna(est_id) and pd.notna(lat) and pd.notna(lon):\n",
    "                if est_id not in estaciones_2024:\n",
    "                    estaciones_2024[est_id] = (lat, lon)\n",
    "\n",
    "    df_estaciones_2024 = pd.DataFrame([\n",
    "        {\"id_estacion\": est_id, \"lat\": lat, \"lon\": lon}\n",
    "        for est_id, (lat, lon) in estaciones_2024.items()\n",
    "    ])\n",
    "    df_estaciones_2024[\"geometry\"] = df_estaciones_2024.apply(lambda row: Point(row[\"lon\"], row[\"lat\"]), axis=1)\n",
    "    gdf_estaciones_2024 = gpd.GeoDataFrame(df_estaciones_2024, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    # ----------------------\n",
    "    # PLOT\n",
    "    # ----------------------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 12))\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    for barrio, shape in barrios_gdf.groupby(\"nombre\"):\n",
    "        shape.plot(ax=ax1, color=color_dict[barrio], edgecolor=\"black\", linewidth=0.5)\n",
    "    gdf_estaciones_todas.plot(ax=ax1, color=\"black\", edgecolor=\"white\", markersize=40, alpha=0.9)\n",
    "    ax1.set_title(\"üìç Todas las estaciones (2020‚Äì2024)\", fontsize=18)\n",
    "    ax1.set_xlabel(\"Longitud\", fontsize=13)\n",
    "    ax1.set_ylabel(\"Latitud\", fontsize=13)\n",
    "    ax1.axis(\"equal\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    for barrio, shape in barrios_gdf.groupby(\"nombre\"):\n",
    "        shape.plot(ax=ax2, color=color_dict[barrio], edgecolor=\"black\", linewidth=0.5)\n",
    "    gdf_estaciones_2024.plot(ax=ax2, color=\"black\", edgecolor=\"white\", markersize=40, alpha=0.9)\n",
    "    ax2.set_title(\"üìç Estaciones activas en 2024\", fontsize=18)\n",
    "    ax2.set_xlabel(\"Longitud\", fontsize=13)\n",
    "    ax2.set_ylabel(\"Latitud\", fontsize=13)\n",
    "    ax2.axis(\"equal\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    handles = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor=color_dict[barrio],\n",
    "               markeredgecolor='black', markersize=10, label=barrio)\n",
    "        for barrio in barrios_unicos\n",
    "    ]\n",
    "    fig.legend(handles=handles, title=\"Barrios\", loc=\"center left\", bbox_to_anchor=(1.01, 0.5), fontsize=\"small\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def analisis_usuarios(test=False, path_test=None):\n",
    "    anios_recorridos = [2020, 2021, 2022, 2023, 2024]\n",
    "    anios_usuarios_nuevos = [2015, 2016, 2017, 2018, 2019]\n",
    "\n",
    "    # === 1. Usuarios de recorridos\n",
    "    if test:\n",
    "        if not path_test or not os.path.exists(path_test):\n",
    "            print(\"‚ö†Ô∏è Path de test inv√°lido o no encontrado.\")\n",
    "            return\n",
    "        print(f\"üß™ Modo test: analizando usuarios en {path_test}\")\n",
    "        df_usuarios_recorridos = pd.read_csv(path_test, usecols=[\"id_usuario\"])\n",
    "        df_usuarios_recorridos = df_usuarios_recorridos.dropna().astype({\"id_usuario\": int})\n",
    "        df_usuarios_recorridos[\"a√±o_recorrido\"] = \"TEST\"\n",
    "    else:\n",
    "        usuarios_recorridos = []\n",
    "        for anio in anios_recorridos:\n",
    "            path = f\"data/recorridos/processed/trips_{anio}_pr.csv\"\n",
    "            if os.path.exists(path):\n",
    "                df = pd.read_csv(path, usecols=[\"id_usuario\"])\n",
    "                df = df.dropna().astype({\"id_usuario\": int})\n",
    "                df[\"a√±o_recorrido\"] = anio\n",
    "                usuarios_recorridos.append(df)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No se encontr√≥ archivo de recorridos {anio}\")\n",
    "        df_usuarios_recorridos = pd.concat(usuarios_recorridos, ignore_index=True)\n",
    "\n",
    "    # === 2. Usuarios registrados 2020‚Äì2024\n",
    "    usuarios_registrados = []\n",
    "    for anio in anios_recorridos:\n",
    "        path = f\"data/usuarios/processed/usuarios_ecobici_{anio}_limpio.csv\"\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path, usecols=[\"ID_usuario\"])\n",
    "            df = df.dropna().astype({\"ID_usuario\": int})\n",
    "            usuarios_registrados.append(df)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No se encontr√≥ archivo de usuarios {anio}\")\n",
    "    df_usuarios_registrados = pd.concat(usuarios_registrados, ignore_index=True)\n",
    "    usuarios_2020_2024_set = set(df_usuarios_registrados[\"ID_usuario\"].unique())\n",
    "\n",
    "    # === 3. Usuarios no registrados en 2020‚Äì2024\n",
    "    df_no_registrados = df_usuarios_recorridos[\n",
    "        ~df_usuarios_recorridos[\"id_usuario\"].isin(usuarios_2020_2024_set)\n",
    "    ].copy()\n",
    "\n",
    "    # === 4. Usuarios registrados 2015‚Äì2019\n",
    "    usuarios_extra = []\n",
    "    for anio in anios_usuarios_nuevos:\n",
    "        path = f\"data/new_data/processed/usuarios_ecobici_{anio}_limpio.csv\"\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path, dtype=str)\n",
    "            df.columns = [col.replace('\"', '') for col in df.columns]\n",
    "            df = df.applymap(lambda x: x.replace('\"', '') if isinstance(x, str) else x)\n",
    "            df[\"ID_usuario\"] = df[\"ID_usuario\"].astype(int)\n",
    "            df[\"a√±o_archivo\"] = anio\n",
    "            usuarios_extra.append(df[[\"ID_usuario\", \"a√±o_archivo\"]])\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No se encontr√≥ archivo de usuarios nuevos {anio}\")\n",
    "    df_usuarios_extra = pd.concat(usuarios_extra, ignore_index=True)\n",
    "    usuarios_2015_2019_set = set(df_usuarios_extra[\"ID_usuario\"].unique())\n",
    "\n",
    "    # === 5. An√°lisis\n",
    "    if test:\n",
    "        usuarios_en_test = df_no_registrados[\"id_usuario\"].unique()\n",
    "        total = len(usuarios_en_test)\n",
    "        encontrados = sum(uid in usuarios_2015_2019_set for uid in usuarios_en_test)\n",
    "        no_encontrados = total - encontrados\n",
    "        print(\"\\nüîç Resultados (modo test):\")\n",
    "        print(f\"Usuarios en test no registrados en 2020‚Äì2024: {total}\")\n",
    "        print(f\"Encontrados en 2015‚Äì2019: {encontrados}\")\n",
    "        print(f\"Siguen sin aparecer: {no_encontrados}\")\n",
    "    else:\n",
    "        resultados = []\n",
    "        for anio in sorted(df_no_registrados[\"a√±o_recorrido\"].unique()):\n",
    "            usuarios_en_anio = df_no_registrados[df_no_registrados[\"a√±o_recorrido\"] == anio][\"id_usuario\"].unique()\n",
    "            total = len(usuarios_en_anio)\n",
    "            encontrados = sum(uid in usuarios_2015_2019_set for uid in usuarios_en_anio)\n",
    "            no_encontrados = total - encontrados\n",
    "            resultados.append({\n",
    "                \"a√±o_recorrido\": anio,\n",
    "                \"usuarios_faltantes_2020_2024\": total,\n",
    "                \"encontrados_en_2015_2019\": encontrados,\n",
    "                \"siguen_sin_aparecer\": no_encontrados\n",
    "            })\n",
    "\n",
    "        df_resultado = pd.DataFrame(resultados)\n",
    "        print(\"üîç Usuarios de recorridos no encontrados en 2020‚Äì2024, pero s√≠ en 2015‚Äì2019:\")\n",
    "        print(df_resultado)\n",
    "    # === 6. Filas generadas por usuarios totalmente desconocidos\n",
    "    usuarios_totalmente_desconocidos = set(df_no_registrados[\"id_usuario\"]) - usuarios_2015_2019_set\n",
    "\n",
    "    df_filas_desconocidas = df_usuarios_recorridos[\n",
    "        df_usuarios_recorridos[\"id_usuario\"].isin(usuarios_totalmente_desconocidos)\n",
    "    ]\n",
    "\n",
    "    if test:\n",
    "        print(f\"\\nüìä Filas generadas por usuarios totalmente desconocidos (ni 2015‚Äì2019 ni 2020‚Äì2024): {len(df_filas_desconocidas)}\")\n",
    "    else:\n",
    "        conteo_filas_por_anio = (\n",
    "            df_filas_desconocidas.groupby(\"a√±o_recorrido\")\n",
    "            .size()\n",
    "            .reset_index(name=\"filas_totales_usuarios_desconocidos\")\n",
    "        )\n",
    "        print(\"\\nüìä Filas generadas por usuarios que no aparecen en ning√∫n archivo (ni 2015‚Äì2019 ni 2020‚Äì2024):\")\n",
    "        print(conteo_filas_por_anio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdd7f1",
   "metadata": {},
   "source": [
    "# SUPER FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_entrada = \"data/recorridos/raw/trips_2024.csv\"\n",
    "ruta_salida = \"data/recorridos/processed/trips_2024_pr.csv\"\n",
    "meses = [9, 10, 11, 12]\n",
    "\n",
    "df_final, total = procesar_recorridos_anio(ruta_entrada, ruta_salida, meses)\n",
    "\n",
    "limpiar_y_guardar_recorridos()\n",
    "\n",
    "archivos = [\n",
    "    \"data/recorridos/processed/trips_2020_pr.csv\",\n",
    "    \"data/recorridos/processed/trips_2021_pr.csv\",\n",
    "    \"data/recorridos/processed/trips_2022_pr.csv\",\n",
    "    \"data/recorridos/processed/trips_2023_pr.csv\",\n",
    "    \"data/recorridos/processed/trips_2024_pr.csv\",\n",
    "]\n",
    "\n",
    "limpiar_y_normalizar_archivos(archivos, funciones)\n",
    "\n",
    "archivos_corregir = {\n",
    "    2020: \"data/recorridos/processed/trips_2020_pr.csv\",\n",
    "    2021: \"data/recorridos/processed/trips_2021_pr.csv\",\n",
    "    2022: \"data/recorridos/processed/trips_2022_pr.csv\",\n",
    "    2023: \"data/recorridos/processed/trips_2023_pr.csv\",\n",
    "}\n",
    "\n",
    "tabla, estaciones_por_anio = analizar_presencia_estaciones(archivos, [2020, 2021, 2022, 2023, 2024]])\n",
    "mostrar_estaciones_faltantes_en_2024(estaciones_por_anio)\n",
    "\n",
    "verificar_consistencia_coordenadas(\"data/recorridos/processed/trips_2024_pr.csv\", anio=2024)\n",
    "corregir_coordenadas_con_base_2024(\"data/recorridos/processed/trips_2024_pr.csv\", archivos_corregir)\n",
    "from src.utils import corregir_longitudes_inconsistentes\n",
    "corregir_longitudes_inconsistentes(\"data/recorridos/processed/trips_2021_pr.csv\")\n",
    "corregir_estaciones_2020()\n",
    "verificar_coordenadas_todos_los_anios([2020, 2021, 2022, 2023])\n",
    "archivos = [\n",
    "    \"data/recorridos/processed/trips_2024_pr.csv\",\n",
    "    \"data/recorridos/processed/trips_2023_pr.csv\",\n",
    "    \"data/recorridos/processed/trips_2022_pr.csv\",\n",
    "    \"data/recorridos/processed/trips_2021_pr.csv\",\n",
    "    \"data/recorridos/processed/trips_2020_pr.csv\"\n",
    "]\n",
    "\n",
    "analyze_barrios()\n",
    "diccionario = construir_diccionario_estaciones(archivos)\n",
    "print(f\"‚úÖ Diccionario construido con {len(diccionario)} estaciones.\")\n",
    "df_estaciones = obtener_o_construir_df_estaciones(archivos)\n",
    "asignar_barrios_a_datasets(df_estaciones, anios=[2020, 2021, 2022, 2023, 2024])\n",
    "estaciones_graf()\n",
    "analisis_usuarios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esto para test\n",
    "\n",
    "verificar_coordenadas_todos_los_anios(test = True, path_csv=\"pathdeltest\")\n",
    "df_estaciones = obtener_o_construir_df_estaciones(None)\n",
    "asignar_barrios_a_datasets(df_estaciones, test=True, path_test=\"pathtest\")\n",
    "analisis_usuarios(test=True, path_test=\"pathtest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf73ad4",
   "metadata": {},
   "source": [
    "# ARMADO DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def insumos_modelado(test=False, path_test=None):\n",
    "    \"\"\"\n",
    "    Crea el dataset modelado, funcionando para modo completo (2020‚Äì2024) o test individual.\n",
    "    \"\"\"\n",
    "    # === CARGA DE VIAJES\n",
    "    if test:\n",
    "        if not path_test or not os.path.exists(path_test):\n",
    "            raise FileNotFoundError(\"‚ùå Path de test inv√°lido.\")\n",
    "        print(f\"üß™ Modo test: usando archivo {path_test}\")\n",
    "        df_viajes = pd.read_csv(path_test)\n",
    "    else:\n",
    "        print(\"üì• Cargando viajes completos 2020‚Äì2024...\")\n",
    "        recorridos_dir = \"data/recorridos/processed\"\n",
    "        df_viajes = pd.concat([\n",
    "            pd.read_csv(os.path.join(recorridos_dir, f))\n",
    "            for f in sorted(os.listdir(recorridos_dir))\n",
    "            if f.endswith(\".csv\")\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    # === CARGA DE USUARIOS (siempre todos)\n",
    "    print(\"üë§ Cargando todos los usuarios (2015‚Äì2024)...\")\n",
    "    usuarios_dir_1 = \"data/usuarios/processed\"\n",
    "    usuarios_dir_2 = \"data/new_data/processed\"\n",
    "    archivos_usuarios = []\n",
    "\n",
    "    for carpeta in [usuarios_dir_1, usuarios_dir_2]:\n",
    "        archivos_usuarios += [\n",
    "            os.path.join(carpeta, f)\n",
    "            for f in sorted(os.listdir(carpeta))\n",
    "            if f.endswith(\".csv\")\n",
    "        ]\n",
    "\n",
    "    df_usuarios = pd.concat([\n",
    "        pd.read_csv(f) for f in archivos_usuarios\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # === CARGA DE ESTACIONES (con barrios ya asignados)\n",
    "    df_estaciones = pd.read_csv(\"data/estaciones_con_barrios.csv\")\n",
    "\n",
    "    # === LLAMADA A LA FUNCI√ìN PRINCIPAL\n",
    "    from modeling import construir_dataset_modelado_v2\n",
    "    df_modelado = construir_dataset_modelado_v2(df_viajes, df_usuarios, df_estaciones, test=test)\n",
    "\n",
    "    return df_modelado\n",
    "\n",
    "def procesar_columnas_modelado(df_modelado):\n",
    "    # === Estaci√≥n del a√±o ‚Üí int\n",
    "    mapa_estaciones = {\"verano\": 1, \"otono\": 2, \"invierno\": 3, \"primavera\": 4}\n",
    "    df_modelado[\"estacion_del_anio\"] = df_modelado[\"estacion_del_anio\"].map(mapa_estaciones).astype(int)\n",
    "\n",
    "    # === Conversiones de fecha\n",
    "    df_modelado[\"fecha_origen_recorrido\"] = pd.to_datetime(df_modelado[\"fecha_origen_recorrido\"], errors=\"coerce\")\n",
    "    df_modelado[\"fecha_destino_recorrido\"] = pd.to_datetime(df_modelado[\"fecha_destino_recorrido\"], errors=\"coerce\")\n",
    "    df_modelado[\"fecha_intervalo\"] = pd.to_datetime(df_modelado[\"fecha_intervalo\"], errors=\"coerce\")\n",
    "\n",
    "    # === Columnas temporales origen\n",
    "    df_modelado[\"a√±o_origen\"] = df_modelado[\"fecha_origen_recorrido\"].dt.year\n",
    "    df_modelado[\"mes_origen\"] = df_modelado[\"fecha_origen_recorrido\"].dt.month\n",
    "    df_modelado[\"dia_origen\"] = df_modelado[\"fecha_origen_recorrido\"].dt.day\n",
    "    df_modelado[\"hora_origen\"] = df_modelado[\"fecha_origen_recorrido\"].dt.hour\n",
    "    df_modelado[\"minuto_origen\"] = df_modelado[\"fecha_origen_recorrido\"].dt.minute\n",
    "    df_modelado[\"segundo_origen\"] = df_modelado[\"fecha_origen_recorrido\"].dt.second\n",
    "\n",
    "    # === Columnas temporales destino\n",
    "    df_modelado[\"a√±o_destino\"] = df_modelado[\"fecha_destino_recorrido\"].dt.year\n",
    "    df_modelado[\"mes_destino\"] = df_modelado[\"fecha_destino_recorrido\"].dt.month\n",
    "    df_modelado[\"dia_destino\"] = df_modelado[\"fecha_destino_recorrido\"].dt.day\n",
    "    df_modelado[\"hora_destino\"] = df_modelado[\"fecha_destino_recorrido\"].dt.hour\n",
    "    df_modelado[\"minuto_destino\"] = df_modelado[\"fecha_destino_recorrido\"].dt.minute\n",
    "    df_modelado[\"segundo_destino\"] = df_modelado[\"fecha_destino_recorrido\"].dt.second\n",
    "\n",
    "    # === Columnas temporales intervalo\n",
    "    df_modelado[\"a√±o_intervalo\"] = df_modelado[\"fecha_intervalo\"].dt.year\n",
    "    df_modelado[\"mes_intervalo\"] = df_modelado[\"fecha_intervalo\"].dt.month\n",
    "    df_modelado[\"dia_intervalo\"] = df_modelado[\"fecha_intervalo\"].dt.day\n",
    "    df_modelado[\"hora_intervalo\"] = df_modelado[\"fecha_intervalo\"].dt.hour\n",
    "    df_modelado[\"minuto_intervalo\"] = df_modelado[\"fecha_intervalo\"].dt.minute\n",
    "\n",
    "    # === Edad\n",
    "    df_modelado[\"edad_usuario\"] = pd.to_numeric(df_modelado[\"edad_usuario\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "\n",
    "    # === Mapeo de barrios √∫nicos\n",
    "    barrios_unicos = pd.unique(df_modelado[[\"barrio_origen\", \"barrio_destino\"]].values.ravel())\n",
    "    mapa_barrio = {barrio: i for i, barrio in enumerate(barrios_unicos, start=1)}\n",
    "\n",
    "    # Guardar el mapa\n",
    "    with open(\"data/modelado/mapa_barrio.json\", \"w\") as f:\n",
    "        json.dump(mapa_barrio, f, ensure_ascii=False)\n",
    "\n",
    "    df_modelado[\"barrio_origen\"] = df_modelado[\"barrio_origen\"].map(mapa_barrio)\n",
    "    df_modelado[\"barrio_destino\"] = df_modelado[\"barrio_destino\"].map(mapa_barrio)\n",
    "\n",
    "    # === Modelo de bicicleta\n",
    "    df_modelado[\"modelo_bicicleta\"] = df_modelado[\"modelo_bicicleta\"].map({\"ICONIC\": 1, \"FIT\": 0}).astype(int)\n",
    "\n",
    "    # === Eliminar columnas no necesarias\n",
    "    df_modelado.drop(columns=[\n",
    "        \"hora_dia\",\n",
    "        \"fecha_origen_recorrido\",\n",
    "        \"fecha_destino_recorrido\",\n",
    "        \"fecha_intervalo\"\n",
    "    ], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    return df_modelado\n",
    "\n",
    "def generar_atributos_estaciones(\n",
    "    path_estaciones: str = \"data/new_data/raw/nuevas-estaciones-bicicletas-publicas.csv\",\n",
    "    path_ciclovias: str = \"data/new_data/raw/ciclovias.geojson\",\n",
    "    output_path: str = \"data/new_data/processed/atributos_estaciones.csv\"\n",
    ") -> None:\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from shapely.geometry import Point, LineString, MultiLineString\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Cargar estaciones y ciclov√≠as\n",
    "    df_est = pd.read_csv(path_estaciones)\n",
    "    gdf_ciclovias = gpd.read_file(path_ciclovias)\n",
    "\n",
    "    # Convertir estaciones a GeoDataFrame\n",
    "    gdf_est = gpd.GeoDataFrame(\n",
    "        df_est,\n",
    "        geometry=gpd.points_from_xy(df_est[\"longitud\"], df_est[\"latitud\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(epsg=3857)\n",
    "\n",
    "    # Ciclov√≠as a misma proyecci√≥n\n",
    "    gdf_ciclovias = gdf_ciclovias.to_crs(epsg=3857)\n",
    "    ciclovias_list = list(gdf_ciclovias.geometry)\n",
    "    ciclovias_union = gdf_ciclovias.unary_union\n",
    "\n",
    "    # Distancia a la ciclov√≠a m√°s cercana\n",
    "    gdf_est[\"dist_ciclovia_m\"] = gdf_est.geometry.apply(lambda p: p.distance(ciclovias_union))\n",
    "\n",
    "    # Longitud de ciclov√≠as en un radio de 200m\n",
    "    longitudes = []\n",
    "    for est_geom in gdf_est.geometry:\n",
    "        buffer = est_geom.buffer(200)\n",
    "        total_length = 0.0\n",
    "        for ciclovia in ciclovias_list:\n",
    "            try:\n",
    "                inter = ciclovia.intersection(buffer)\n",
    "                if inter.is_empty:\n",
    "                    continue\n",
    "                if isinstance(inter, (LineString, MultiLineString)):\n",
    "                    total_length += inter.length\n",
    "            except Exception:\n",
    "                continue\n",
    "        longitudes.append(total_length)\n",
    "    gdf_est[\"ciclo_len_200m\"] = longitudes\n",
    "\n",
    "    # Dejar solo columnas √∫tiles\n",
    "    df_out = gdf_est[[\n",
    "        \"id\", \"dist_ciclovia_m\", \"ciclo_len_200m\"\n",
    "    ]].rename(columns={\"id\": \"id_estacion_origen\"})\n",
    "\n",
    "    df_out.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Atributos de estaciones guardados en: {output_path}\")\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def final_prep(\n",
    "    path_modelado: str,\n",
    "    path_output: str = \"data/modelado/ds_modelado_FUNCIONA.csv\",\n",
    "    path_atributos_estacion: str = \"data/new_data/processed/atributos_estaciones.csv\",\n",
    "    path_barrios: str = \"data/estaciones_con_barrios.csv\",\n",
    "    path_mapa_barrio: str = \"data/modelado/mapa_barrio.json\"\n",
    ") -> None:\n",
    "    df = pd.read_csv(path_modelado)\n",
    "\n",
    "    df[\"fecha_intervalo\"] = pd.to_datetime(dict(\n",
    "        year=df[\"a√±o_intervalo\"],\n",
    "        month=df[\"mes_intervalo\"],\n",
    "        day=df[\"dia_intervalo\"],\n",
    "        hour=df[\"hora_intervalo\"],\n",
    "        minute=df[\"minuto_intervalo\"]\n",
    "    ))\n",
    "\n",
    "    fecha_min = df[\"fecha_intervalo\"].min()\n",
    "    fecha_max = df[\"fecha_intervalo\"].max()\n",
    "    intervalos = pd.date_range(start=fecha_min, end=fecha_max, freq=\"30min\")\n",
    "    estaciones = df[\"id_estacion_origen\"].unique()\n",
    "\n",
    "    df_regular = pd.MultiIndex.from_product(\n",
    "        [estaciones, intervalos],\n",
    "        names=[\"id_estacion_origen\", \"fecha_intervalo\"]\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    df_lags = df[[\n",
    "        \"id_estacion_origen\", \"fecha_intervalo\",\n",
    "        \"N_arribos_intervalo\", \"N_salidas_intervalo\"\n",
    "    ]]\n",
    "    df_regular = df_regular.merge(df_lags, on=[\"id_estacion_origen\", \"fecha_intervalo\"], how=\"left\")\n",
    "    df_regular[[\"N_arribos_intervalo\", \"N_salidas_intervalo\"]] = df_regular[[\"N_arribos_intervalo\", \"N_salidas_intervalo\"]].fillna(0)\n",
    "\n",
    "    for lag in [1, 2, 3]:\n",
    "        df_regular[f\"arribos_lag{lag}\"] = df_regular.groupby(\"id_estacion_origen\")[\"N_arribos_intervalo\"].shift(lag)\n",
    "    for lag in [1, 2]:\n",
    "        df_regular[f\"salidas_lag{lag}\"] = df_regular.groupby(\"id_estacion_origen\")[\"N_salidas_intervalo\"].shift(lag)\n",
    "\n",
    "    df_regular[\"arribos_rolling7\"] = (\n",
    "        df_regular.groupby(\"id_estacion_origen\")[\"N_arribos_intervalo\"]\n",
    "            .transform(lambda x: x.shift(1).rolling(window=7).mean())\n",
    "    )\n",
    "    df_regular[\"arribos_ultima_hora\"] = (\n",
    "        df_regular.groupby(\"id_estacion_origen\")[\"N_arribos_intervalo\"]\n",
    "            .transform(lambda x: x.shift(1).rolling(window=2).sum())\n",
    "    )\n",
    "\n",
    "    lag_cols = [col for col in df_regular.columns if \"lag\" in col or \"rolling\" in col]\n",
    "    df_regular[lag_cols] = df_regular[lag_cols].fillna(-1)\n",
    "\n",
    "    df = df.merge(\n",
    "        df_regular[[\"id_estacion_origen\", \"fecha_intervalo\"] + lag_cols],\n",
    "        on=[\"id_estacion_origen\", \"fecha_intervalo\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    df[\"fecha_origen\"] = pd.to_datetime(dict(\n",
    "        year=df[\"a√±o_origen\"],\n",
    "        month=df[\"mes_origen\"],\n",
    "        day=df[\"dia_origen\"],\n",
    "        hour=df[\"hora_origen\"],\n",
    "        minute=df[\"minuto_origen\"]\n",
    "    ))\n",
    "    df = df.sort_values([\"id_usuario\", \"fecha_origen\"]).copy()\n",
    "\n",
    "    df[\"ultima_estacion_origen_usuario\"] = df.groupby(\"id_usuario\")[\"id_estacion_origen\"].shift(1).fillna(-1).astype(int)\n",
    "    df[\"ultima_estacion_destino_usuario\"] = df.groupby(\"id_usuario\")[\"id_estacion_destino\"].shift(1).fillna(-1).astype(int)\n",
    "    df[\"fecha_ultimo_viaje_usuario\"] = df.groupby(\"id_usuario\")[\"fecha_origen\"].shift(1)\n",
    "    df[\"tiempo_desde_ultimo_viaje_usuario\"] = (\n",
    "        (df[\"fecha_origen\"] - df[\"fecha_ultimo_viaje_usuario\"]).dt.total_seconds() / 60\n",
    "    ).fillna(-1)\n",
    "\n",
    "    df_barrios = pd.read_csv(path_barrios)\n",
    "    with open(path_mapa_barrio, \"r\") as f:\n",
    "        mapa_barrio = json.load(f)\n",
    "\n",
    "    df = df.merge(\n",
    "        df_barrios.rename(columns={\n",
    "            \"id_estacion\": \"ultima_estacion_origen_usuario\",\n",
    "            \"barrio\": \"barrio_ultima_estacion_origen_usuario\"\n",
    "        }),\n",
    "        on=\"ultima_estacion_origen_usuario\", how=\"left\"\n",
    "    )\n",
    "    df = df.merge(\n",
    "        df_barrios.rename(columns={\n",
    "            \"id_estacion\": \"ultima_estacion_destino_usuario\",\n",
    "            \"barrio\": \"barrio_ultima_estacion_destino_usuario\"\n",
    "        }),\n",
    "        on=\"ultima_estacion_destino_usuario\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    df[\"barrio_ultima_estacion_origen_usuario\"] = df[\"barrio_ultima_estacion_origen_usuario\"].map(mapa_barrio).fillna(-1).astype(int)\n",
    "    df[\"barrio_ultima_estacion_destino_usuario\"] = df[\"barrio_ultima_estacion_destino_usuario\"].map(mapa_barrio).fillna(-1).astype(int)\n",
    "\n",
    "    for lag in [1, 2, 3]:\n",
    "        df[f\"estacion_origen_viejo_us_lag{lag}\"] = df.groupby(\"id_usuario\")[\"id_estacion_origen\"].shift(lag).fillna(-1).astype(int)\n",
    "        df[f\"estacion_destino_viejo_us_lag{lag}\"] = df.groupby(\"id_usuario\")[\"id_estacion_destino\"].shift(lag).fillna(-1).astype(int)\n",
    "\n",
    "    df[\"estacion_origen_viejo_us\"] = df[\"estacion_origen_viejo_us_lag1\"]\n",
    "    df[\"estacion_destino_viejo_us\"] = df[\"estacion_destino_viejo_us_lag1\"]\n",
    "\n",
    "    df_atributos = pd.read_csv(path_atributos_estacion)\n",
    "    df = df.merge(df_atributos, on=\"id_estacion_origen\", how=\"left\")\n",
    "\n",
    "    df.to_csv(path_output, index=False)\n",
    "    print(f\"‚úÖ Dataset final guardado en {path_output}\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def limpiar_y_guardar_parquet(\n",
    "    path_csv: str,\n",
    "    path_parquet: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Limpia columnas innecesarias y guarda el dataset en formato Parquet.\n",
    "    \n",
    "    Par√°metros:\n",
    "    - path_csv: ruta al CSV de entrada\n",
    "    - path_parquet: ruta de salida en formato .parquet\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path_csv):\n",
    "        print(f\"‚ùå No se encontr√≥ el archivo: {path_csv}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(path_csv)\n",
    "\n",
    "    # Eliminar columnas innecesarias si existen\n",
    "    df = df.drop(columns=[\n",
    "        \"lat_x\", \"lon_x\", \"lat_y\", \"lon_y\", \"fecha_ultimo_viaje_usuario\"\n",
    "    ], errors=\"ignore\")\n",
    "\n",
    "    # Rellenar columnas de ciclov√≠as\n",
    "    df[\"ciclo_len_200m\"] = df[\"ciclo_len_200m\"].fillna(0.0)\n",
    "    df[\"dist_ciclovia_m\"] = df[\"dist_ciclovia_m\"].fillna(9999.0)\n",
    "\n",
    "    # Guardar\n",
    "    df.to_parquet(path_parquet, index=False)\n",
    "    print(f\"‚úÖ Parquet guardado en: {path_parquet}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb7444",
   "metadata": {},
   "source": [
    "# PARA LAS FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616cfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelado = insumos_modelado()\n",
    "df_modelado = procesar_columnas_modelado(df_modelado)\n",
    "df_modelado.to_csv(\"data/modelado/ds_modelado.csv\", index=False)\n",
    "generar_atributos_estaciones()\n",
    "final_prep(\n",
    "    path_modelado=\"data/modelado/ds_modelado.csv\",\n",
    "    path_output=\"data/modelado/ds_modelado_final.csv\"\n",
    ")\n",
    "limpiar_y_guardar_parquet(\n",
    "    path_csv=\"data/modelado/ds_modelado_final.csv\",\n",
    "    path_parquet=\"data/modelado/ds_modelado.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a57808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTO ES TEST\n",
    "df_test = insumos_modelado(test=True, path_test=\"data/recorridos/processed/trips_2023_pr.csv\")\n",
    "df_modelado_test = procesar_columnas_modelado(df_test)\n",
    "df_modelado_test.to_csv(\"data/modelado/ds_modelado_test.csv\", index=False)\n",
    "final_prep(\n",
    "    path_modelado=\"data/test/ds_modelado_test.csv\",\n",
    "    path_output=\"data/test/ds_modelado_test_final.csv\"\n",
    ")\n",
    "limpiar_y_guardar_parquet(\n",
    "    path_csv=\"data/test/ds_modelado_test_final.csv\",\n",
    "    path_parquet=\"data/test/ds_modelado_test.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from modeling import (\n",
    "    insumos_modelado,\n",
    "    procesar_columnas_modelado,\n",
    "    construir_dataset_modelado_v2,\n",
    "    final_prep,\n",
    "    limpiar_y_guardar_parquet\n",
    ")\n",
    "\n",
    "def armar_dataset_modelado(test=False, path_test=None):\n",
    "    if test:\n",
    "        if not path_test or not os.path.exists(path_test):\n",
    "            print(\"‚ùå Path de test inv√°lido o no existe.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\nüß™ Generando dataset de TEST...\")\n",
    "        df_test = insumos_modelado(test=True, path_test=path_test)\n",
    "        df_modelado_test = procesar_columnas_modelado(df_test)\n",
    "\n",
    "        path_csv = \"data/test/ds_modelado_test.csv\"\n",
    "        path_csv_final = \"data/test/ds_modelado_test_final.csv\"\n",
    "        path_parquet = \"data/test/ds_modelado_test.parquet\"\n",
    "    \n",
    "        df_modelado_test.to_csv(path_csv, index=False)\n",
    "\n",
    "        final_prep(\n",
    "            path_modelado=path_csv,\n",
    "            path_output=path_csv_final\n",
    "        )\n",
    "\n",
    "        limpiar_y_guardar_parquet(\n",
    "            path_csv=path_csv_final,\n",
    "            path_parquet=path_parquet\n",
    "        )\n",
    "        print(\"‚úÖ Dataset de test generado correctamente.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nüß± Generando dataset de TRAIN (a√±os completos)...\")\n",
    "        df_train = insumos_modelado()\n",
    "        df_modelado_train = procesar_columnas_modelado(df_train)\n",
    "\n",
    "        path_csv = \"data/modelado/ds_modelado.csv\"\n",
    "        path_csv_final = \"data/modelado/ds_modelado_FUNCIONA.csv\"\n",
    "        path_parquet = \"data/modelado/ds_modelado_FUNCIONA.parquet\"\n",
    "\n",
    "        df_modelado_train.to_csv(path_csv, index=False)\n",
    "\n",
    "        final_prep(\n",
    "            path_modelado=path_csv,\n",
    "            path_output=path_csv_final\n",
    "        )\n",
    "\n",
    "        limpiar_y_guardar_parquet(\n",
    "            path_csv=path_csv_final,\n",
    "            path_parquet=path_parquet\n",
    "        )\n",
    "        print(\"‚úÖ Dataset de entrenamiento generado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38322241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling import armar_dataset_modelado\n",
    "from utils import preprocesamiento_completo\n",
    "# Para entrenamiento (modo completo)\n",
    "preprocesamiento_completo()\n",
    "\n",
    "# Para test\n",
    "preprocesamiento_completo(test=True, path_test=\"data/recorridos/processed/trips_2023_pr.csv\")\n",
    "\n",
    "\n",
    "armar_dataset_modelado(test=False)\n",
    "armar_dataset_modelado(test=True, path_test=\"pathtest\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
